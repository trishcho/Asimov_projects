{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researcher - TRISHUL CHOWDHURY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(r'C:\\Users\\Trishul Chowdhury\\Desktop\\final code\\ljmu_implementation\\final draft'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all dependencies.\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34660, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trishul Chowdhury\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_csv(r\"C:\\Users\\Trishul Chowdhury\\Desktop\\final Thesis\\final code\\ljmu_implementation\\final draft\\amazon_200.csv\")\n",
    "print(meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.rename(columns={\"name\":\"Product Name\",\"reviews.doRecommend\":\"recommendation_indicator\",\"reviews.rating\":\"reviews_rating\",\"reviews.text\":\"reviews_text\"},inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34660"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_review_data=meta\n",
    "len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Percentage_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <td>6760</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asins</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keys</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.date</th>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.dateAdded</th>\n",
       "      <td>10621</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.dateSeen</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.didPurchase</th>\n",
       "      <td>34659</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommendation_indicator</th>\n",
       "      <td>594</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.id</th>\n",
       "      <td>34659</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <td>529</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_rating</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_text</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.title</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.userCity</th>\n",
       "      <td>34660</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <td>34660</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews.username</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0  Percentage_null\n",
       "id                            0              0.0\n",
       "Product Name               6760             20.0\n",
       "asins                         2              0.0\n",
       "brand                         0              0.0\n",
       "categories                    0              0.0\n",
       "keys                          0              0.0\n",
       "manufacturer                  0              0.0\n",
       "reviews.date                 39              0.0\n",
       "reviews.dateAdded         10621             31.0\n",
       "reviews.dateSeen              0              0.0\n",
       "reviews.didPurchase       34659            100.0\n",
       "recommendation_indicator    594              2.0\n",
       "reviews.id                34659            100.0\n",
       "reviews.numHelpful          529              2.0\n",
       "reviews_rating               33              0.0\n",
       "reviews.sourceURLs            0              0.0\n",
       "reviews_text                  1              0.0\n",
       "reviews.title                 5              0.0\n",
       "reviews.userCity          34660            100.0\n",
       "reviews.userProvince      34660            100.0\n",
       "reviews.username              2              0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=pd.DataFrame(amazon_review_data.isnull().sum())\n",
    "a[\"Percentage_null\"]=((a[0]/len(meta))*100).round()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Percentage_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <td>6760</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommendation_indicator</th>\n",
       "      <td>594</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_text</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_rating</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  Percentage_null\n",
       "Product Name              6760             20.0\n",
       "brand                        0              0.0\n",
       "categories                   0              0.0\n",
       "recommendation_indicator   594              2.0\n",
       "reviews_text                 1              0.0\n",
       "reviews_rating              33              0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=a.loc[[\"Product Name\",\"brand\",\"categories\",\"recommendation_indicator\",\"reviews_text\",\"reviews_rating\"]]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Percentage_null</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <td>6760</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommendation_indicator</th>\n",
       "      <td>594</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_text</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviews_rating</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  Percentage_null\n",
       "Features Name                                  \n",
       "Product Name              6760             20.0\n",
       "brand                        0              0.0\n",
       "categories                   0              0.0\n",
       "recommendation_indicator   594              2.0\n",
       "reviews_text                 1              0.0\n",
       "reviews_rating              33              0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.index.names = ['Features Name']\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sym spell package\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect\n",
    "#from language_detector import detect_language\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Product Name', 'asins', 'brand', 'categories', 'keys',\n",
       "       'manufacturer', 'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen',\n",
       "       'reviews.didPurchase', 'recommendation_indicator', 'reviews.id',\n",
       "       'reviews.numHelpful', 'reviews_rating', 'reviews.sourceURLs',\n",
       "       'reviews_text', 'reviews.title', 'reviews.userCity',\n",
       "       'reviews.userProvince', 'reviews.username'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        This product so far has not disappointed. My c...\n",
       "1        great for beginner or experienced person. Boug...\n",
       "2        Inexpensive tablet for him to use and learn on...\n",
       "3        I've had my Fire HD 8 two weeks now and I love...\n",
       "4        I bought this for my grand daughter when she c...\n",
       "                               ...                        \n",
       "34655    This is not appreciably faster than any other ...\n",
       "34656    Amazon should include this charger with the Ki...\n",
       "34657    Love my Kindle Fire but I am really disappoint...\n",
       "34658    I was surprised to find it did not come with a...\n",
       "34659    to spite the fact that i have nothing but good...\n",
       "Name: reviews_text, Length: 34660, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"reviews_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.rename(columns={'reviews_text': 'reviews'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(meta[\"reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  This product so far has not disappointed. My c...\n",
       "1  great for beginner or experienced person. Boug...\n",
       "2  Inexpensive tablet for him to use and learn on..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34660, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#### sentence level preprocess ####\n",
    "###################################\n",
    "\n",
    "# lowercase + base filter\n",
    "# some basic normalization\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (s is English)\n",
    "    \"\"\"\n",
    "\n",
    "    # some reviews are actually english but biased toward french\n",
    "    return detect(s) in {'English', 'French','Spanish','Chinese'}\n",
    "\n",
    "\n",
    "###############################\n",
    "#### word level preprocess ####\n",
    "###############################\n",
    "\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "\n",
    "# selecting nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "    return w_list_fixed\n",
    "\n",
    "\n",
    "# stemming if doing word-wise\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    return [p_stemmer.stem(word) for word in w_list]\n",
    "\n",
    "\n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "\n",
    "stop_words = (list(\n",
    "    set(get_stop_words('en'))\n",
    "    |set(get_stop_words('es'))\n",
    "    |set(get_stop_words('de'))\n",
    "    |set(get_stop_words('it'))\n",
    "    |set(get_stop_words('ca'))\n",
    "    #|set(get_stop_words('cy'))\n",
    "    |set(get_stop_words('pt'))\n",
    "    #|set(get_stop_words('tl'))\n",
    "    |set(get_stop_words('pl'))\n",
    "    #|set(get_stop_words('et'))\n",
    "    |set(get_stop_words('da'))\n",
    "    |set(get_stop_words('ru'))\n",
    "    #|set(get_stop_words('so'))\n",
    "    |set(get_stop_words('sv'))\n",
    "    |set(get_stop_words('sk'))\n",
    "    #|set(get_stop_words('cs'))\n",
    "    |set(get_stop_words('nl'))\n",
    "    #|set(get_stop_words('sl'))\n",
    "    #|set(get_stop_words('no'))\n",
    "    #|set(get_stop_words('zh-cn'))\n",
    "))\n",
    "\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]\n",
    "\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s = f_base(rw)\n",
    "    #if not f_lan(s):\n",
    "    #    return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)\n",
    "    w_list = f_noun(w_list)\n",
    "    w_list = f_typo(w_list)\n",
    "    w_list = f_stem(w_list)\n",
    "    w_list = f_stopw(w_list)\n",
    "\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs, samp_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    if not samp_size:\n",
    "        samp_size = 100\n",
    "\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = []  # index of sample selected\n",
    "    #     samp = list(range(100))\n",
    "    samp = np.random.choice(n_docs, samp_size)\n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34660, 1)                                                  reviews\n",
      "0      This product so far has not disappointed. My c...\n",
      "1      great for beginner or experienced person. Boug...\n",
      "2      Inexpensive tablet for him to use and learn on...\n",
      "3      I've had my Fire HD 8 two weeks now and I love...\n",
      "4      I bought this for my grand daughter when she c...\n",
      "...                                                  ...\n",
      "34655  This is not appreciably faster than any other ...\n",
      "34656  Amazon should include this charger with the Ki...\n",
      "34657  Love my Kindle Fire but I am really disappoint...\n",
      "34658  I was surprised to find it did not come with a...\n",
      "34659  to spite the fact that i have nothing but good...\n",
      "\n",
      "[34660 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data = df \n",
    "print(data.shape,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  This product so far has not disappointed. My c...\n",
       "1  great for beginner or experienced person. Boug..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n"
     ]
    }
   ],
   "source": [
    "data = data.fillna('')  # only the comments has NaN's\n",
    "rws = data[\"reviews\"]\n",
    "sentences, token_lists, idx_in = preprocess(rws,samp_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49379"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my husband is a truck driver and being on the road away from home, he likes to have shows and movies to watch. this has so much space for him to download things to watch.',\n",
       " 'wound up finding one at best buy when no one else had any. i absolutely have found my new favorite gadget in the echo and have found so many fun things it can do to help organize my life.',\n",
       " \"it was an okay purchase for my 6 year old. i definitely wouldn't pouches it for myself. not much you can do on it.\",\n",
       " 'easiest device to read a book on. easy to read in any light and easy to hold.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49379"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['husband',\n",
       "  'truck',\n",
       "  'driver',\n",
       "  'road',\n",
       "  'home',\n",
       "  'show',\n",
       "  'movi',\n",
       "  'space',\n",
       "  'thing'],\n",
       " ['wound', 'buy', 'gadget', 'echo', 'thing', 'life'],\n",
       " ['purchas', 'year'],\n",
       " ['devic', 'book'],\n",
       " ['tablet', 'parent', 'christma'],\n",
       " ['present', 'eye'],\n",
       " ['first', 'option', 'port', 'plenti', 'storag'],\n",
       " ['echo',\n",
       "  'day',\n",
       "  'music',\n",
       "  'dish',\n",
       "  'voic',\n",
       "  'command',\n",
       "  'hand',\n",
       "  'morn',\n",
       "  'eye',\n",
       "  'weather',\n",
       "  'alex',\n",
       "  'plan',\n",
       "  'day'],\n",
       " ['husband',\n",
       "  'father',\n",
       "  'day',\n",
       "  'temperatur',\n",
       "  'morn',\n",
       "  'set',\n",
       "  'timer',\n",
       "  'granni',\n",
       "  'electron',\n",
       "  'alarm',\n",
       "  'music',\n",
       "  'granni',\n",
       "  'joke',\n",
       "  'day',\n",
       "  'technolog'],\n",
       " ['tablet', 'asp', 'custom', 'ose', 'nexu']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lists[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## PRE PROCESSING DONE FOR THIS MODEL ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    Autoencoder for learning latent space representation\n",
    "    architecture simplified for only one hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.his = None\n",
    "\n",
    "    def _compile(self, input_dim):\n",
    "        \"\"\"\n",
    "        compile the computational graph\n",
    "        \"\"\"\n",
    "        input_vec = Input(shape=(input_dim,))\n",
    "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
    "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
    "        self.autoencoder = Model(input_vec, decoded)\n",
    "        self.encoder = Model(input_vec, encoded)\n",
    "        encoded_input = Input(shape=(self.latent_dim,))\n",
    "        decoder_layer = self.autoencoder.layers[-1]\n",
    "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
    "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
    "\n",
    "    def fit(self, X):\n",
    "        if not self.autoencoder:\n",
    "            self._compile(X.shape[1])\n",
    "        X_train, X_test = train_test_split(X)\n",
    "        self.his = self.autoencoder.fit(X_train, X_train,\n",
    "                                        epochs=200,\n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        validation_data=(X_test, X_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation utility class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils class \n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_topic_words(token_lists, labels, k=None):\n",
    "    \"\"\"\n",
    "    get top words within each topic from clustering results\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(np.unique(labels))\n",
    "    topics = ['' for _ in range(k)]\n",
    "    for i, c in enumerate(token_lists):\n",
    "        topics[labels[i]] += (' ' + ' '.join(c))\n",
    "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
    "    # get sorted word counts\n",
    "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
    "    # get topics\n",
    "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
    "\n",
    "    return topics\n",
    "\n",
    "def get_coherence(model, token_lists, measure='c_v'):\n",
    "    \"\"\"\n",
    "    Get model coherence from gensim.models.coherencemodel\n",
    "    :param model: Topic_Model object\n",
    "    :param token_lists: token lists of docs\n",
    "    :param topics: topics as top words\n",
    "    :param measure: coherence metrics\n",
    "    :return: coherence score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    else:\n",
    "        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
    "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    return cm.get_coherence()\n",
    "\n",
    "def get_silhouette(model):\n",
    "    \"\"\"\n",
    "    Get silhouette score from model\n",
    "    :param model: Topic_Model object\n",
    "    :return: silhouette score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    lbs = model.cluster_model.labels_\n",
    "    vec = model.vec[model.method]\n",
    "    return silhouette_score(vec, lbs)\n",
    "\n",
    "def plot_proj(embedding, lbs):\n",
    "    \"\"\"\n",
    "    Plot UMAP embeddings\n",
    "    :param embedding: UMAP (or other) embeddings\n",
    "    :param lbs: labels\n",
    "    \"\"\"\n",
    "    n = len(embedding)\n",
    "    counter = Counter(lbs)\n",
    "    for i in range(len(np.unique(lbs))):\n",
    "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
    "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n",
    "\n",
    "\n",
    "def visualize(model):\n",
    "    \"\"\"\n",
    "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
    "    :param model: Topic_Model object\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    reducer = umap.UMAP()\n",
    "    print('Calculating UMAP projection ...')\n",
    "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
    "    print('Calculating UMAP projection. Done!')\n",
    "    plot_proj(vec_umap, model.cluster_model.labels_)\n",
    "    dr = '/working/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('/working/2D_vis')\n",
    "\n",
    "def get_wordcloud(model, token_lists, topic):\n",
    "    \"\"\"\n",
    "    Get word cloud of each topic from fitted model\n",
    "    :param model: Topic_Model object\n",
    "    :param sentences: preprocessed sentences from docs\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    print('Getting wordcloud for topic {} ...'.format(topic))\n",
    "    lbs = model.cluster_model.labels_\n",
    "    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=560,\n",
    "                          background_color='white', collocations=False,\n",
    "                          min_font_size=10).generate(tokens)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize=(8, 5.6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    dr = '/working/{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('/working' + '/Topic' + str(topic) + '_wordcloud')\n",
    "    print('Getting wordcloud for topic {}. Done!'.format(topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def preprocess(docs, samp_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    if not samp_size:\n",
    "        samp_size = 100\n",
    "\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = []  # word level preprocessed\n",
    "    idx_in = []  # index of sample selected\n",
    "    #     samp = list(range(100))\n",
    "    samp = np.random.choice(n_docs, samp_size)\n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in\n",
    "\n",
    "\n",
    "# define model object\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=10, method='TFIDF'):\n",
    "        \"\"\"\n",
    "        :param k: number of topics\n",
    "        :param method: method chosen for the topic model\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
    "            raise Exception('Invalid method!')\n",
    "        self.k = k\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        #         self.stopwords = None\n",
    "        self.cluster_model = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "        \n",
    "        #######################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.gamma = 20  # parameter for relative importance to BERT vector\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        \n",
    "        self.method = method\n",
    "        self.AE = None\n",
    "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def vectorize(self, sentences, token_lists, method=None):\n",
    "        \"\"\"\n",
    "        Get vecotr representations from selected methods\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        self.dictionary = corpora.Dictionary(token_lists)\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'TFIDF':\n",
    "            print('Getting vector representations for TF-IDF ...')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            vec = tfidf.fit_transform(sentences)\n",
    "            print('Getting vector representations for TF-IDF. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))\n",
    "                for i in range(n_doc):\n",
    "                    # get the distribution for the i-th document in corpus\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'BERT':\n",
    "\n",
    "            print('Getting vector representations for BERT ...')\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
    "            print('Getting vector representations for BERT. Done!')\n",
    "            return vec\n",
    "\n",
    "             \n",
    "        elif method == 'LDA_BERT':\n",
    "        #else:\n",
    "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
    "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
    "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
    "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
    "            if not self.AE:\n",
    "                self.AE = Autoencoder()\n",
    "                print('Fitting Autoencoder ...')\n",
    "                self.AE.fit(vec_ldabert)\n",
    "                print('Fitting Autoencoder Done!')\n",
    "            vec = self.AE.encoder.predict(vec_ldabert)\n",
    "            return vec\n",
    "\n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        \"\"\"\n",
    "        Fit the topic model for selected method given the preprocessed data\n",
    "        :docs: list of documents, each doc is preprocessed as tokens\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        # Default clustering method\n",
    "        if m_clustering is None:\n",
    "            m_clustering = KMeans\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        if not self.dictionary:\n",
    "            self.dictionary = corpora.Dictionary(token_lists)\n",
    "            # convert tokenized documents into a document-term matrix\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        ####################################################\n",
    "        #### Getting ldamodel or vector representations ####\n",
    "        ####################################################\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "                print('Fitting LDA Done!')\n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)\n",
    "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
    "            self.cluster_model.fit(self.vec[method])\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
    "        \"\"\"\n",
    "        Predict topics for new_documents\n",
    "        \"\"\"\n",
    "        # Default as False\n",
    "        out_of_sample = out_of_sample is not None\n",
    "\n",
    "        if out_of_sample:\n",
    "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "            if self.method != 'LDA':\n",
    "                vec = self.vectorize(sentences, token_lists)\n",
    "                print(vec)\n",
    "        else:\n",
    "            corpus = self.corpus\n",
    "            vec = self.vec.get(self.method, None)\n",
    "\n",
    "        if self.method == \"LDA\":\n",
    "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
    "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
    "                                    corpus)))\n",
    "        else:\n",
    "            lbs = self.cluster_model.predict(vec)\n",
    "        return lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import *\n",
    "#from utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "import argparse\n",
    "\n",
    "#def model(): #:if __name__ == '__main__':\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    method = \"LDA_BERT\"\n",
    "    samp_size = 50000\n",
    "    ntopic = 10\n",
    "    \n",
    "    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n",
    "\n",
    "    \n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    data = df \n",
    "    data = data.fillna('')  # only the comments has NaN's\n",
    "    rws = data[\"reviews.text\"]\n",
    "    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n",
    "    # Define the topic model object\n",
    "    #tm = Topic_Model(k = 10), method = TFIDF)\n",
    "    tm = Topic_Model(k = ntopic, method = method)\n",
    "    # Fit the topic model by chosen method\n",
    "    tm.fit(sentences, token_lists)\n",
    "    # Evaluate using metrics\n",
    "    with open(\"A:/MLAI_MSC/ljmu_implementation/{}.file\".format(tm.id), \"wb\") as f:\n",
    "        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
    "    print('Silhouette Score:', get_silhouette(tm))\n",
    "    # visualize and save img\n",
    "    visualize(tm)\n",
    "    for i in range(tm.k):\n",
    "        get_wordcloud(tm, token_lists, i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing block wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"LDA_BERT\"\n",
    "samp_size = 50000\n",
    "ntopic = 10\n",
    "tm = Topic_Model(k = ntopic, method = method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering embeddings ...\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Getting vector representations for BERT ...\n"
     ]
    }
   ],
   "source": [
    "tm.fit(sentences, token_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
    "print('Silhouette Score:', get_silhouette(tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coherence:', get_coherence(tm, token_lists, 'c_v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tm.vec[tm.method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
