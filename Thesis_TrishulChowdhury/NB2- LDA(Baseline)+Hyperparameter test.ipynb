{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researcher - TRISHUL CHOWDHURY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trishul Chowdhury\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Trishul Chowdhury\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('amazon_200.csv', error_bad_lines=False);\n",
    "data_text = data[['reviews.text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34660"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        reviews.text  index\n",
       "0  This product so far has not disappointed. My c...      0\n",
       "1  great for beginner or experienced person. Boug...      1\n",
       "2  Inexpensive tablet for him to use and learn on...      2\n",
       "3  I've had my Fire HD 8 two weeks now and I love...      3\n",
       "4  I bought this for my grand daughter when she c...      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Trishul\n",
      "[nltk_data]     Chowdhury\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['I', 'got', 'this', 'Kindle', 'as', 'a', 'Christmas', 'gift', 'for', 'a', 'friend.', \"Couldn't\", 'pass', 'up', 'the', 'Black', 'Friday', 'price', 'on', 'this', 'thing!', 'Works', 'great.', 'She', 'loves', 'it.', 'Success!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['kindl', 'christma', 'gift', 'friend', 'couldn', 'pass', 'black', 'friday', 'price', 'thing', 'work', 'great', 'love', 'success']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['reviews.text'].fillna('').astype(str).map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [product, disappoint, children, love, like, ab...\n",
       "1     [great, beginn, experi, person, buy, gift, love]\n",
       "2    [inexpens, tablet, learn, step, nabi, thrill, ...\n",
       "3    [week, love, tablet, great, valu, prime, membe...\n",
       "4    [buy, grand, daughter, come, visit, user, ente...\n",
       "5    [amazon, inch, tablet, perfect, size, purchas,...\n",
       "6    [great, read, nice, light, weight, price, poin...\n",
       "7    [give, christma, gift, inlaw, husband, uncl, l...\n",
       "8    [great, devic, read, book, like, link, borrow,...\n",
       "9                    [love, order, book, read, reader]\n",
       "Name: reviews.text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [product, disappoint, children, love, like, ab...\n",
       "1         [great, beginn, experi, person, buy, gift, love]\n",
       "2        [inexpens, tablet, learn, step, nabi, thrill, ...\n",
       "3        [week, love, tablet, great, valu, prime, membe...\n",
       "4        [buy, grand, daughter, come, visit, user, ente...\n",
       "                               ...                        \n",
       "34655    [appreci, faster, higher, charger, kindl, kind...\n",
       "34656    [amazon, includ, charger, kindl, fact, charg, ...\n",
       "34657    [love, kindl, disappoint, kindl, power, fast, ...\n",
       "34658    [surpris, come, type, charg, cord, purchas, sp...\n",
       "34659    [spite, fact, good, thing, amazon, anth, get, ...\n",
       "Name: reviews.text, Length: 34660, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abil\n",
      "1 children\n",
      "2 content\n",
      "3 control\n",
      "4 disappoint\n",
      "5 eas\n",
      "6 like\n",
      "7 love\n",
      "8 monitor\n",
      "9 product\n",
      "10 beginn\n"
     ]
    }
   ],
   "source": [
    "## Bag of words \n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (74, 1),\n",
       " (101, 1),\n",
       " (114, 1),\n",
       " (136, 1),\n",
       " (182, 1),\n",
       " (185, 1),\n",
       " (187, 1),\n",
       " (338, 1),\n",
       " (415, 1),\n",
       " (652, 1),\n",
       " (1450, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 (\"love\") appears 1 time.\n",
      "Word 13 (\"gift\") appears 1 time.\n",
      "Word 14 (\"great\") appears 1 time.\n",
      "Word 74 (\"price\") appears 1 time.\n",
      "Word 101 (\"christma\") appears 1 time.\n",
      "Word 114 (\"kindl\") appears 1 time.\n",
      "Word 136 (\"work\") appears 1 time.\n",
      "Word 182 (\"couldn\") appears 1 time.\n",
      "Word 185 (\"thing\") appears 1 time.\n",
      "Word 187 (\"friend\") appears 1 time.\n",
      "Word 338 (\"black\") appears 1 time.\n",
      "Word 415 (\"friday\") appears 1 time.\n",
      "Word 652 (\"pass\") appears 1 time.\n",
      "Word 1450 (\"success\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.35211031156819533), (1, 0.33520998250946255), (2, 0.3416445918662697), (3, 0.2610370507872522), (4, 0.3622034810513108), (5, 0.35852949058523387), (6, 0.16718112604987245), (7, 0.10388620564883835), (8, 0.5005859796176302), (9, 0.172418891517214)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.059*\"echo\" + 0.045*\"alexa\" + 0.036*\"music\" + 0.030*\"love\" + 0.028*\"great\" '\n",
      "  '+ 0.027*\"cabl\" + 0.020*\"home\" + 0.018*\"work\" + 0.016*\"play\" + '\n",
      "  '0.016*\"question\"'),\n",
      " (1,\n",
      "  '0.039*\"work\" + 0.034*\"tablet\" + 0.024*\"perfect\" + 0.020*\"firetv\" + '\n",
      "  '0.020*\"purchas\" + 0.017*\"screen\" + 0.016*\"buy\" + 0.016*\"replac\" + '\n",
      "  '0.015*\"kindl\" + 0.014*\"want\"'),\n",
      " (2,\n",
      "  '0.071*\"product\" + 0.063*\"great\" + 0.044*\"recommend\" + 0.041*\"easi\" + '\n",
      "  '0.039*\"purchas\" + 0.037*\"work\" + 0.028*\"love\" + 0.021*\"buy\" + 0.020*\"good\" '\n",
      "  '+ 0.020*\"best\"'),\n",
      " (3,\n",
      "  '0.045*\"love\" + 0.034*\"book\" + 0.031*\"buy\" + 0.026*\"read\" + 0.024*\"year\" + '\n",
      "  '0.021*\"thing\" + 0.019*\"like\" + 0.019*\"play\" + 0.016*\"time\" + 0.016*\"list\"'),\n",
      " (4,\n",
      "  '0.080*\"great\" + 0.044*\"tablet\" + 0.036*\"price\" + 0.034*\"good\" + '\n",
      "  '0.032*\"amazon\" + 0.026*\"qualiti\" + 0.021*\"sound\" + 0.018*\"product\" + '\n",
      "  '0.015*\"speaker\" + 0.015*\"money\"'),\n",
      " (5,\n",
      "  '0.035*\"batteri\" + 0.026*\"life\" + 0.016*\"good\" + 0.016*\"need\" + 0.016*\"buy\" '\n",
      "  '+ 0.014*\"long\" + 0.014*\"alexa\" + 0.013*\"song\" + 0.012*\"love\" + '\n",
      "  '0.012*\"like\"'),\n",
      " (6,\n",
      "  '0.027*\"amazon\" + 0.023*\"devic\" + 0.022*\"stick\" + 0.018*\"like\" + '\n",
      "  '0.016*\"great\" + 0.016*\"stream\" + 0.016*\"work\" + 0.014*\"app\" + '\n",
      "  '0.011*\"better\" + 0.009*\"weather\"'),\n",
      " (7,\n",
      "  '0.068*\"read\" + 0.062*\"kindl\" + 0.044*\"light\" + 0.032*\"love\" + 0.030*\"book\" '\n",
      "  '+ 0.030*\"easi\" + 0.022*\"great\" + 0.017*\"screen\" + 0.015*\"reader\" + '\n",
      "  '0.015*\"paperwhit\"'),\n",
      " (8,\n",
      "  '0.052*\"amazon\" + 0.039*\"prime\" + 0.026*\"easi\" + 0.020*\"remot\" + '\n",
      "  '0.017*\"need\" + 0.017*\"movi\" + 0.016*\"good\" + 0.015*\"great\" + 0.013*\"stream\" '\n",
      "  '+ 0.013*\"music\"'),\n",
      " (9,\n",
      "  '0.102*\"love\" + 0.041*\"great\" + 0.038*\"kid\" + 0.031*\"buy\" + 0.028*\"easi\" + '\n",
      "  '0.028*\"tablet\" + 0.024*\"watch\" + 0.020*\"movi\" + 0.019*\"game\" + '\n",
      "  '0.019*\"famili\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = processed_docs\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.36919537637695854\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                     | 0/540 [04:31<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 540/540 [17:38:55<00:00, 117.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.059*\"echo\" + 0.045*\"alexa\" + 0.036*\"music\" + 0.030*\"love\" + 0.029*\"great\" + 0.028*\"cabl\" + 0.020*\"home\" + 0.018*\"work\" + 0.016*\"play\" + 0.016*\"question\"\n",
      "Topic: 1 \n",
      "Words: 0.038*\"work\" + 0.034*\"tablet\" + 0.024*\"perfect\" + 0.020*\"purchas\" + 0.020*\"firetv\" + 0.017*\"screen\" + 0.016*\"buy\" + 0.016*\"replac\" + 0.015*\"kindl\" + 0.014*\"want\"\n",
      "Topic: 2 \n",
      "Words: 0.071*\"product\" + 0.063*\"great\" + 0.044*\"recommend\" + 0.041*\"easi\" + 0.039*\"purchas\" + 0.037*\"work\" + 0.028*\"love\" + 0.021*\"buy\" + 0.020*\"good\" + 0.020*\"best\"\n",
      "Topic: 3 \n",
      "Words: 0.045*\"love\" + 0.034*\"book\" + 0.031*\"buy\" + 0.026*\"read\" + 0.024*\"year\" + 0.021*\"thing\" + 0.019*\"like\" + 0.019*\"play\" + 0.016*\"time\" + 0.016*\"list\"\n",
      "Topic: 4 \n",
      "Words: 0.080*\"great\" + 0.044*\"tablet\" + 0.036*\"price\" + 0.034*\"good\" + 0.032*\"amazon\" + 0.026*\"qualiti\" + 0.021*\"sound\" + 0.018*\"product\" + 0.015*\"speaker\" + 0.015*\"money\"\n",
      "Topic: 5 \n",
      "Words: 0.035*\"batteri\" + 0.026*\"life\" + 0.016*\"good\" + 0.016*\"buy\" + 0.016*\"need\" + 0.014*\"long\" + 0.014*\"alexa\" + 0.013*\"song\" + 0.013*\"love\" + 0.012*\"like\"\n",
      "Topic: 6 \n",
      "Words: 0.027*\"amazon\" + 0.023*\"devic\" + 0.022*\"stick\" + 0.018*\"like\" + 0.016*\"great\" + 0.016*\"work\" + 0.016*\"stream\" + 0.014*\"app\" + 0.011*\"better\" + 0.009*\"weather\"\n",
      "Topic: 7 \n",
      "Words: 0.068*\"read\" + 0.062*\"kindl\" + 0.044*\"light\" + 0.032*\"love\" + 0.030*\"book\" + 0.029*\"easi\" + 0.022*\"great\" + 0.017*\"screen\" + 0.015*\"reader\" + 0.015*\"paperwhit\"\n",
      "Topic: 8 \n",
      "Words: 0.052*\"amazon\" + 0.039*\"prime\" + 0.025*\"easi\" + 0.020*\"remot\" + 0.017*\"movi\" + 0.017*\"need\" + 0.016*\"good\" + 0.015*\"great\" + 0.014*\"stream\" + 0.013*\"music\"\n",
      "Topic: 9 \n",
      "Words: 0.102*\"love\" + 0.041*\"great\" + 0.038*\"kid\" + 0.031*\"buy\" + 0.029*\"easi\" + 0.028*\"tablet\" + 0.023*\"watch\" + 0.020*\"movi\" + 0.019*\"game\" + 0.019*\"famili\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"read\" + 0.016*\"kindl\" + 0.016*\"easi\" + 0.015*\"book\" + 0.015*\"love\" + 0.014*\"great\" + 0.013*\"good\" + 0.012*\"light\" + 0.011*\"voic\" + 0.009*\"work\"\n",
      "Topic: 1 Word: 0.015*\"kid\" + 0.014*\"love\" + 0.014*\"great\" + 0.011*\"tablet\" + 0.010*\"easi\" + 0.010*\"time\" + 0.010*\"recommend\" + 0.010*\"product\" + 0.009*\"thank\" + 0.008*\"work\"\n",
      "Topic: 2 Word: 0.014*\"firetv\" + 0.011*\"great\" + 0.010*\"pictur\" + 0.009*\"better\" + 0.009*\"work\" + 0.008*\"love\" + 0.008*\"good\" + 0.008*\"stick\" + 0.008*\"read\" + 0.008*\"best\"\n",
      "Topic: 3 Word: 0.036*\"stick\" + 0.015*\"recommend\" + 0.012*\"love\" + 0.012*\"product\" + 0.011*\"great\" + 0.011*\"amazon\" + 0.010*\"purchas\" + 0.010*\"best\" + 0.010*\"cabl\" + 0.010*\"better\"\n",
      "Topic: 4 Word: 0.025*\"firestick\" + 0.020*\"easi\" + 0.018*\"great\" + 0.015*\"tablet\" + 0.012*\"price\" + 0.012*\"love\" + 0.011*\"work\" + 0.010*\"good\" + 0.010*\"recommend\" + 0.010*\"batteri\"\n",
      "Topic: 5 Word: 0.014*\"amazon\" + 0.014*\"great\" + 0.011*\"work\" + 0.011*\"easi\" + 0.011*\"roku\" + 0.010*\"like\" + 0.009*\"tablet\" + 0.009*\"love\" + 0.009*\"devic\" + 0.009*\"channel\"\n",
      "Topic: 6 Word: 0.015*\"love\" + 0.014*\"cabl\" + 0.013*\"gift\" + 0.012*\"buy\" + 0.011*\"great\" + 0.011*\"kodi\" + 0.010*\"amazon\" + 0.009*\"purchas\" + 0.008*\"friend\" + 0.008*\"easi\"\n",
      "Topic: 7 Word: 0.020*\"echo\" + 0.017*\"great\" + 0.017*\"alexa\" + 0.015*\"music\" + 0.015*\"love\" + 0.014*\"work\" + 0.014*\"enjoy\" + 0.013*\"product\" + 0.010*\"good\" + 0.010*\"speaker\"\n",
      "Topic: 8 Word: 0.022*\"love\" + 0.020*\"buy\" + 0.015*\"gift\" + 0.015*\"christma\" + 0.014*\"great\" + 0.013*\"product\" + 0.011*\"easi\" + 0.011*\"tablet\" + 0.011*\"roku\" + 0.010*\"friend\"\n",
      "Topic: 9 Word: 0.027*\"movi\" + 0.024*\"stream\" + 0.023*\"watch\" + 0.017*\"great\" + 0.016*\"amazon\" + 0.015*\"easi\" + 0.015*\"show\" + 0.013*\"work\" + 0.012*\"prime\" + 0.012*\"product\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kindl',\n",
       " 'christma',\n",
       " 'gift',\n",
       " 'friend',\n",
       " 'couldn',\n",
       " 'pass',\n",
       " 'black',\n",
       " 'friday',\n",
       " 'price',\n",
       " 'thing',\n",
       " 'work',\n",
       " 'great',\n",
       " 'love',\n",
       " 'success']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6681009531021118\t \n",
      "Topic: 0.071*\"product\" + 0.063*\"great\" + 0.044*\"recommend\" + 0.041*\"easi\" + 0.039*\"purchas\" + 0.037*\"work\" + 0.028*\"love\" + 0.021*\"buy\" + 0.020*\"good\" + 0.020*\"best\"\n",
      "\n",
      "Score: 0.15862774848937988\t \n",
      "Topic: 0.038*\"work\" + 0.034*\"tablet\" + 0.024*\"perfect\" + 0.020*\"purchas\" + 0.020*\"firetv\" + 0.017*\"screen\" + 0.016*\"buy\" + 0.016*\"replac\" + 0.015*\"kindl\" + 0.014*\"want\"\n",
      "\n",
      "Score: 0.12658506631851196\t \n",
      "Topic: 0.035*\"batteri\" + 0.026*\"life\" + 0.016*\"good\" + 0.016*\"buy\" + 0.016*\"need\" + 0.014*\"long\" + 0.014*\"alexa\" + 0.013*\"song\" + 0.013*\"love\" + 0.012*\"like\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance evaluation by classifying sample document using LDA TF-IDF mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6037395000457764\t \n",
      "Topic: 0.020*\"echo\" + 0.017*\"great\" + 0.017*\"alexa\" + 0.015*\"music\" + 0.015*\"love\" + 0.014*\"work\" + 0.014*\"enjoy\" + 0.013*\"product\" + 0.010*\"good\" + 0.010*\"speaker\"\n",
      "\n",
      "Score: 0.24526943266391754\t \n",
      "Topic: 0.022*\"love\" + 0.020*\"buy\" + 0.015*\"gift\" + 0.015*\"christma\" + 0.014*\"great\" + 0.013*\"product\" + 0.011*\"easi\" + 0.011*\"tablet\" + 0.011*\"roku\" + 0.010*\"friend\"\n",
      "\n",
      "Score: 0.10430815070867538\t \n",
      "Topic: 0.014*\"firetv\" + 0.011*\"great\" + 0.010*\"pictur\" + 0.009*\"better\" + 0.009*\"work\" + 0.008*\"love\" + 0.008*\"good\" + 0.008*\"stick\" + 0.008*\"read\" + 0.008*\"best\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.32300981879234314\t Topic: 0.068*\"read\" + 0.062*\"kindl\" + 0.044*\"light\" + 0.032*\"love\" + 0.030*\"book\"\n",
      "Score: 0.3205356299877167\t Topic: 0.071*\"product\" + 0.063*\"great\" + 0.044*\"recommend\" + 0.041*\"easi\" + 0.039*\"purchas\"\n",
      "Score: 0.21639566123485565\t Topic: 0.052*\"amazon\" + 0.039*\"prime\" + 0.025*\"easi\" + 0.020*\"remot\" + 0.017*\"movi\"\n",
      "Score: 0.0200138371437788\t Topic: 0.102*\"love\" + 0.041*\"great\" + 0.038*\"kid\" + 0.031*\"buy\" + 0.029*\"easi\"\n",
      "Score: 0.020009679719805717\t Topic: 0.035*\"batteri\" + 0.026*\"life\" + 0.016*\"good\" + 0.016*\"buy\" + 0.016*\"need\"\n",
      "Score: 0.02000967413187027\t Topic: 0.045*\"love\" + 0.034*\"book\" + 0.031*\"buy\" + 0.026*\"read\" + 0.024*\"year\"\n",
      "Score: 0.020007513463497162\t Topic: 0.059*\"echo\" + 0.045*\"alexa\" + 0.036*\"music\" + 0.030*\"love\" + 0.029*\"great\"\n",
      "Score: 0.02000698260962963\t Topic: 0.038*\"work\" + 0.034*\"tablet\" + 0.024*\"perfect\" + 0.020*\"purchas\" + 0.020*\"firetv\"\n",
      "Score: 0.020005984231829643\t Topic: 0.080*\"great\" + 0.044*\"tablet\" + 0.036*\"price\" + 0.034*\"good\" + 0.032*\"amazon\"\n",
      "Score: 0.0200052447617054\t Topic: 0.027*\"amazon\" + 0.023*\"devic\" + 0.022*\"stick\" + 0.018*\"like\" + 0.016*\"great\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'I love the kindle. However I just hate their delivery process'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
